{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Using a Pipeline for Text Transformation and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will practice text vectorization to transform text into numerical feature vectors that can be used to train a classifier. You will then see how to use scikit-learn pipelines to chain together these processes into one step. You will:\n",
    "\n",
    "1. Load the book reviews data set.\n",
    "2. Use a single text column as a feature. \n",
    "3. Transform features using a TF-IDF vectorizer. \n",
    "4. Fit a logistic regression model to the transformed features. \n",
    "5. Evaluate the performance of the model using AUC.\n",
    "6. Set up a scikit-learn pipeline to perform the same tasks above. \n",
    "7. Execute the pipeline and verify that the performance is the same.\n",
    "8. Add a grid search to the pipeline to find the optimal hyperparameter configuration.\n",
    "9. Evaluate the performance of the optimal configuration using ROC-AUC.\n",
    "\n",
    "**<font color='red'>Note: some of the code cells in this notebook may take a while to run</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the book review dataset that you worked with in the sentiment analysis demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"bookReviews.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples \n",
    "\n",
    "<b>Task</b>: Create labeled examples from DataFrame `df`. We will have one text feature and one label.  \n",
    "\n",
    "In the code cell below carry out the following steps:\n",
    "\n",
    "* Get the `Positive Review` column from DataFrame `df` and assign it to the variable `y`. This will be our label.\n",
    "* Get the column `Review` from DataFrame `df` and assign it to the variable `X`. This will be our feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Positive Review']\n",
    "X = df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labeled Examples into Training and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement TF-IDF Vectorizer to Transform Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Complete the code in the cell below to implement a TF-IDF transformation on the training and test data.\n",
    "Use the \"Transforming Text For a Classifier\" demo as a guide. Follow the following steps:\n",
    "\n",
    "1. Create a `TfidfVectorizer` object and save it to the variable `tfidf_vectorizer`.\n",
    "2. Call `tfidf_vectorizer.fit()` to fit the vectorizer to the training data `X_train`.\n",
    "3. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the training data `X_train`. Save the result to `X_train_tfidf`.\n",
    "4. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the test data `X_test`. Save the result to `X_test_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a TfidfVectorizer object and save it to the variable 'tfidf_vectorizer'\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer \n",
    "\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# 3. Using the fitted vectorizer, transform the training data \n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "# 4. Using the fitted vectorizer, transform the test data\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fit a Logistic Regression Model to the Transformed Training Data and Evaluate the Model\n",
    "<b>Task</b>: Complete the code cell below to train a logistic regression model using the TF-IDF features, and compute the AUC on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the LogisticRegression model object \n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# 2. Fit the model to the transformed training data\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Use the predict_proba() method to make predictions on the test data \n",
    "\n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "# 4. Compute the area under the ROC curve for the test data. \n",
    "\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "\n",
    "# 5. Compute the size of the resulting feature space \n",
    "\n",
    "len_feature_space = tfidf_vectorizer.vocabulary_\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Experiment with Different Document Frequency Values and Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: The cell below will loop over a range of 'document frequency' values. For each value, it will fit a vectorizer specifying `ngram_range=(1,2)`. It will then fit a logistic regression model to the transformed data and evaluate the results.   \n",
    "\n",
    "Complete the loop in the cell below by \n",
    "\n",
    "1. adding a list containing four document frequency values that you would like to use (e.g. `[1, 10, 100, 1000]`)\n",
    "2. adding the code you wrote above inside the loop. \n",
    "\n",
    "Note: This may take a short while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_df in [1, 10, 100, 1000]: \n",
    "    \n",
    "    print('\\nDocument Frequency Value: {0}'.format(min_df))\n",
    "\n",
    "    # 1. Create a TfidfVectorizer object   \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=min_df)\n",
    "\n",
    "    # 2. Fit the vectorizer to X_train  \n",
    "    \n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    # 3. Using the fitted vectorizer, transform the training data.\n",
    "    \n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "    # 4. Using the fitted vectorizer, transform the test data.\n",
    "    \n",
    "    \n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    # 5. Create the LogisticRegression model object \n",
    "    \n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    \n",
    "    # 6. Fit the model to the transformed training data\n",
    "    \n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # 7. Use the predict_proba() method to make predictions \n",
    "    probability_predicions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    # 8. Using roc_auc_score() function to compute the AUC. \n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    \n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "    \n",
    "\n",
    "    # 9. Compute the size of the resulting feature space. \n",
    "    len_feature_space = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Which document frequency value and feature space produced the best performing model? Do you notice any patterns regarding the number of document frequency values, the feature space and the AUC? Record your findings in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document frequency value 100 produces the best-performing model because all the resulting AUC values are identical. We want to choose the fastest-running model if they're all the same. However, if we select 1000, we wouldn't be able to get any keywords even though the resulting AUC is the same. Choosing 100 is the safest option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Set up a TF-IDF + Logistic Regression Pipeline\n",
    "\n",
    "We will look at a new way to chain together various methods to automate the machine learning workflow. We will use  the scikit-learn `Pipeline` utility. For more information, consult the online [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). First, let's import `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below will use a scikit-learn pipeline to perform TF-IDF vectorization and the fitting of a logistic regression model to the transformed data.\n",
    "\n",
    "\n",
    "\n",
    "<b>Task:</b> In the code cell below, complete the using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin ML pipeline...')\n",
    "\n",
    "# 1. Define the list of steps:\n",
    "s = [\n",
    "        (\"vectorizer\", TfidfVectorizer(ngram_range=(1,2), min_df=10)),\n",
    "        (\"model\", LogisticRegression(max_iter=200))\n",
    "    ]\n",
    "\n",
    "# 2. Define the pipeline:\n",
    "model_pipeline = Pipeline(steps=s)\n",
    "\n",
    "# We can use the pipeline the way would would use a model object \n",
    "# when fitting the model on the training data and testing on the test data:\n",
    "\n",
    "# 3. Fit the pipeline to the training data\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions on the test data\n",
    "# Save the second column to the variable 'probability_predictions'\n",
    "\n",
    "probability_predictions = model_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('End pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of our model. \n",
    "\n",
    "<b>Task</b>: In the code cell below, call the function `roc_auc_score()` with arguments `y_test` and `probability_predictions`. Save the results to the variable `auc_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance by computing the AUC\n",
    "\n",
    "auc_score = roc_auc_score(y_test, probability_predictions)\n",
    "\n",
    "print('AUC on the test data: {:.4f}'.format(auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, scikit-learn gives you the ability to provide a pipeline object as an argument to a function. One such function is `plot_roc_curve()`. You'll see in the online [documentation](https://scikit-learn.org/0.23/modules/generated/sklearn.metrics.plot_roc_curve.html) that this function can take a pipeline (estimator) as an argument. Calling `plot_roc_curve()` with the pipeline and the test data will accomplish the same tasks as steps 3 and 4 in the code cell above.\n",
    "\n",
    "Let's import the function and try it out.\n",
    "\n",
    "<b>Task:</b> Call `plot_roc_curve()` with the following three arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(model_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in newer versions of scikit-learn, this function has been replaced by [RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Perform a GridSearchCV on the Pipeline to Find the Best Hyperparameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will perform a grid search on the pipeline object `model_pipeline` to find the hyperparameter configuration for hyperparameter $C$ (for the logistic regression) and for the $ngram\\_range$ (for the TF-IDF vectorizer) that result in the best cross-validation score.\n",
    "\n",
    "<b>Task:</b> Define a parameter grid to pass to `GridSearchCV()`. Recall that the parameter grid is a dictionary. Name the dictionary `param_grid`.\n",
    "\n",
    "\n",
    "\n",
    "Note that following:\n",
    "\n",
    "When running a grid search on a pipelines, the hyperparameter names you specify in the parameter grid are the names of the pipeline items (the descriptive names you provided to the items in the pipeline) followed by two underscores, followed by the actual hyperparameter names. \n",
    "\n",
    "\n",
    "We named the the classifier `model` and the vectorizer `vectorizer`. \n",
    "\n",
    "Since we named our classifier `model`, the hyperparameter name for $C$ that you would specify as they key in `param_grid` is `model__C`. You can find a list containing possible pipeline hyperparameter names you can use by running the code the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'vectorizer__ngram_range':[(1,1), (1,2)], 'model__C':[0.1,1,10]}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Run a grid search on the pipeline.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Grid Search...')\n",
    "\n",
    "# 1. Run a Grid Search with 3-fold cross-validation and assign the output to the \n",
    "\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, param_grid, cv=3, scoring='roc_auc', verbose=2)\n",
    "\n",
    "# 2. Fit the model (grid_LR) on the training data and assign the fitted model to the \n",
    "\n",
    "\n",
    "grid_search = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the best pipeline configuration that was determined by the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Print the best hyperparameters by accessing them by using the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the past, after we obtained the best hyperparameter values from a grid search, we re-trained a model with these values in order to evaluate the performance. This time we will do something different. Just as we can pass a pipeline object directly to `plot_roc_curve()` to evaluate the model, we can pass `grid_search.best_estimator_` to the function `plot_roc_curve()` to evaluate the model. We also pass in the test data (`X_test` and `y_test`). This allows the test data to be passed through the entire pipeline, using the best hyperparameter values.\n",
    "\n",
    "\n",
    "<b>Task</b>: In the code cell below plot the ROC curve and compute the AUC by calling the function `plot_roc_curve()` with the arguments `grid_search.best_estimator_` and the test data (`X_test` and  `y_test`). Note that you can simply just pass `grid_search` to the function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(grid_search.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
